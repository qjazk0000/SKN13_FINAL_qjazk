import os
import re
from typing import List, Dict, Tuple
from django.conf import settings
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer
from openai import OpenAI

# í”„ë¡¬í”„íŠ¸ ë¡œë” ì§ì ‘ êµ¬í˜„
def load_prompt(path: str, *, default: str = "") -> str:
    """í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    if not os.path.exists(path):
        return default
    try:
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        print(f"WARNING: Failed to load prompt from {path}: {e}")
        return default

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
try:
    prompt_path = '/app/config/system_prompt.md'
    SYSTEM_PROMPT = load_prompt(prompt_path, 
                                default="ë‹¹ì‹ ì€ í•œêµ­ì¸í„°ë„·ì§„í¥ì›ì˜ ê·œì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.")
except FileNotFoundError:
    SYSTEM_PROMPT = "ë‹¹ì‹ ì€ í•œêµ­ì¸í„°ë„·ì§„í¥ì›ì˜ ê·œì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
    print("WARNING: system_prompt.md not found, using default prompt")

# QdrantClientë¥¼ ì „ì—­ìœ¼ë¡œ ìƒì„± (ì—°ê²° ì¬ì‚¬ìš©)
_qdrant_client = None
_embedder = None

def _get_qdrant_client():
    global _qdrant_client
    if _qdrant_client is None:
        _qdrant_client = QdrantClient(host=settings.QDRANT_HOST, port=settings.QDRANT_PORT)
    return _qdrant_client

def _get_embedder():
    global _embedder
    if _embedder is None:
        _embedder = SentenceTransformer("nlpai-lab/KoE5")
    return _embedder

def _extract_keywords(query: str) -> List[str]:
    """ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    # í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ (ê·œì •, ì¡°í•­, ì ˆì°¨ ë“±)
    keywords = re.findall(r'[ê°€-í£]+', query)
    # ë¶ˆìš©ì–´ ì œê±°
    stopwords = ['ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì˜', 'ì™€', 'ê³¼', 'ë„', 'ëŠ”', 'ì€']
    keywords = [kw for kw in keywords if kw not in stopwords and len(kw) > 1]
    return keywords[:5]  # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©

def _extract_document_type(query: str) -> str:
    """ì§ˆë¬¸ì—ì„œ ë¬¸ì„œ ìœ í˜• ì¶”ì¶œ"""
    query_lower = query.lower()
    
    # ë¬¸ì„œ ìœ í˜• ë§¤í•‘
    type_mapping = {
        'ì •ê´€': ['ì •ê´€', 'ê¸°ë³¸ë²•', 'ê·¼ë³¸ë²•'],
        'ê·œì •': ['ê·œì •', 'ìš´ì˜ê·œì •', 'ê´€ë¦¬ê·œì •'],
        'ê·œì¹™': ['ê·œì¹™', 'ì„¸ë¶€ê·œì¹™', 'ì‹¤í–‰ê·œì¹™'],
        'ì§€ì¹¨': ['ì§€ì¹¨', 'ê°€ì´ë“œë¼ì¸', 'ìš´ì˜ì§€ì¹¨'],
        'ì¸ì‚¬': ['ì¸ì‚¬', 'ì±„ìš©', 'ê¸‰ì—¬', 'íœ´ê°€', 'ìœ¡ì•„íœ´ì§', 'ì—°ì°¨'],
        'íšŒê³„': ['íšŒê³„', 'ì˜ˆì‚°', 'ê°ì‚¬', 'ì¬ì •'],
        'ë³´ì•ˆ': ['ë³´ì•ˆ', 'ì •ë³´ë³´í˜¸', 'ê°œì¸ì •ë³´'],
        'ê¸°ìˆ ': ['ê¸°ìˆ ', 'IT', 'ì •ë³´í™”', 'ì‹œìŠ¤í…œ']
    }
    
    for doc_type, keywords in type_mapping.items():
        if any(keyword in query_lower for keyword in keywords):
            return doc_type
    
    return "ì¼ë°˜"

def _classify_document_by_domain(filename: str) -> Dict[str, str]:
    """íŒŒì¼ëª… ê¸°ë°˜ìœ¼ë¡œ ì—…ë¬´ ë„ë©”ì¸ì„ ì •êµí•˜ê²Œ ë¶„ë¥˜"""
    
    filename_lower = filename.lower()
    
    # 1ì°¨ ë¶„ë¥˜: ë¬¸ì„œ ê³„ì¸µ
    if filename_lower.startswith('1_'):
        document_level = 'ì •ê´€'
        level_description = 'ê¸°ë³¸ë²•ë ¹'
    elif filename_lower.startswith('2_'):
        document_level = 'ê·œì •'
        if 'ìš´ì˜' in filename_lower:
            level_description = 'ìš´ì˜ê·œì •'
        elif 'ê´€ë¦¬' in filename_lower:
            level_description = 'ê´€ë¦¬ê·œì •'
        else:
            level_description = 'ì¼ë°˜ê·œì •'
    elif filename_lower.startswith('3_'):
        document_level = 'ê·œì¹™'
        if 'ì¸ì‚¬' in filename_lower:
            level_description = 'ì¸ì‚¬ê·œì¹™'
        elif 'ê¸‰ì—¬' in filename_lower:
            level_description = 'ê¸‰ì—¬ê·œì¹™'
        elif 'ì·¨ì—…' in filename_lower:
            level_description = 'ì·¨ì—…ê·œì¹™'
        elif 'ì¶œì¥' in filename_lower:
            level_description = 'ì¶œì¥ê·œì¹™'
        elif 'ê³„ì•½' in filename_lower:
            level_description = 'ê³„ì•½ê·œì¹™'
        elif 'ë³´ì•ˆ' in filename_lower:
            level_description = 'ë³´ì•ˆê·œì¹™'
        elif 'ê¸°ìˆ ' in filename_lower:
            level_description = 'ê¸°ìˆ ê·œì¹™'
        else:
            level_description = 'ì¼ë°˜ê·œì¹™'
    elif filename_lower.startswith('4_'):
        document_level = 'ì§€ì¹¨'
        if 'ìš´ì˜' in filename_lower:
            level_description = 'ìš´ì˜ì§€ì¹¨'
        elif 'ì—…ë¬´' in filename_lower:
            level_description = 'ì—…ë¬´ì§€ì¹¨'
        else:
            level_description = 'ì¼ë°˜ì§€ì¹¨'
    else:
        document_level = 'ê¸°íƒ€'
        level_description = 'ê¸°íƒ€'
    
    # 2ì°¨ ë¶„ë¥˜: ì—…ë¬´ ë„ë©”ì¸ (ìƒì„¸)
    domain = 'ì¼ë°˜ì—…ë¬´'
    subdomain = 'ê¸°íƒ€'
    
    # ì¸ì‚¬ê´€ë¦¬ ë„ë©”ì¸
    if any(keyword in filename_lower for keyword in ['ì¸ì‚¬', 'ê¸‰ì—¬', 'ì·¨ì—…', 'ì¶œì¥', 'ì±„ìš©', 'ë³µë¬´', 'êµìœ¡', 'í›ˆë ¨']):
        domain = 'ì¸ì‚¬ê´€ë¦¬'
        if 'ì¸ì‚¬' in filename_lower:
            subdomain = 'ì¸ì‚¬ì •ì±…'
        elif 'ê¸‰ì—¬' in filename_lower:
            subdomain = 'ê¸‰ì—¬ê´€ë¦¬'
        elif 'ì·¨ì—…' in filename_lower:
            subdomain = 'ì·¨ì—…ê´€ë¦¬'
        elif 'ì¶œì¥' in filename_lower:
            subdomain = 'ì¶œì¥ê´€ë¦¬'
        elif 'ì±„ìš©' in filename_lower:
            subdomain = 'ì±„ìš©ê´€ë¦¬'
        elif 'ë³µë¬´' in filename_lower:
            subdomain = 'ë³µë¬´ê´€ë¦¬'
        elif 'êµìœ¡' in filename_lower or 'í›ˆë ¨' in filename_lower:
            subdomain = 'êµìœ¡í›ˆë ¨'
    
    # ì¬ë¬´ê´€ë¦¬ ë„ë©”ì¸
    elif any(keyword in filename_lower for keyword in ['íšŒê³„', 'ê°ì‚¬', 'ìì‚°', 'ê³„ì•½', 'ìˆ˜ìˆ˜ë£Œ', 'ì˜ˆì‚°', 'ì¬ì •']):
        domain = 'ì¬ë¬´ê´€ë¦¬'
        if 'íšŒê³„' in filename_lower:
            subdomain = 'íšŒê³„ê´€ë¦¬'
        elif 'ê°ì‚¬' in filename_lower:
            subdomain = 'ê°ì‚¬ê´€ë¦¬'
        elif 'ìì‚°' in filename_lower:
            subdomain = 'ìì‚°ê´€ë¦¬'
        elif 'ê³„ì•½' in filename_lower:
            subdomain = 'ê³„ì•½ê´€ë¦¬'
        elif 'ìˆ˜ìˆ˜ë£Œ' in filename_lower:
            subdomain = 'ìˆ˜ìˆ˜ë£Œê´€ë¦¬'
    
    # ë³´ì•ˆê´€ë¦¬ ë„ë©”ì¸
    elif any(keyword in filename_lower for keyword in ['ë³´ì•ˆ', 'ì •ë³´ë³´í˜¸', 'ê°œì¸ì •ë³´', 'ë¯¼ì›', 'ì‹ ê³ ']):
        domain = 'ë³´ì•ˆê´€ë¦¬'
        if 'ë³´ì•ˆ' in filename_lower:
            subdomain = 'ë³´ì•ˆì •ì±…'
        elif 'ì •ë³´ë³´í˜¸' in filename_lower:
            subdomain = 'ì •ë³´ë³´í˜¸'
        elif 'ê°œì¸ì •ë³´' in filename_lower:
            subdomain = 'ê°œì¸ì •ë³´ë³´í˜¸'
        elif 'ë¯¼ì›' in filename_lower or 'ì‹ ê³ ' in filename_lower:
            subdomain = 'ë¯¼ì›ì‹ ê³ '
    
    # ê¸°ìˆ ê´€ë¦¬ ë„ë©”ì¸
    elif any(keyword in filename_lower for keyword in ['ê¸°ìˆ ', 'IT', 'ì •ë³´í™”', 'ì „ìì„œëª…', 'ì‹œìŠ¤í…œ']):
        domain = 'ê¸°ìˆ ê´€ë¦¬'
        if 'ê¸°ìˆ ' in filename_lower:
            subdomain = 'ê¸°ìˆ ì •ì±…'
        elif 'IT' in filename_lower or 'ì •ë³´í™”' in filename_lower:
            subdomain = 'ì •ë³´í™”ê´€ë¦¬'
        elif 'ì „ìì„œëª…' in filename_lower:
            subdomain = 'ì „ìì„œëª…ê´€ë¦¬'
        elif 'ì‹œìŠ¤í…œ' in filename_lower:
            subdomain = 'ì‹œìŠ¤í…œê´€ë¦¬'
    
    # í–‰ì •ê´€ë¦¬ ë„ë©”ì¸
    elif any(keyword in filename_lower for keyword in ['ë¬¸ì„œ', 'ìë£Œ', 'ê¸°ë¡', 'í™ë³´']):
        domain = 'í–‰ì •ê´€ë¦¬'
        if 'ë¬¸ì„œ' in filename_lower:
            subdomain = 'ë¬¸ì„œê´€ë¦¬'
        elif 'ìë£Œ' in filename_lower:
            subdomain = 'ìë£Œê´€ë¦¬'
        elif 'ê¸°ë¡' in filename_lower:
            subdomain = 'ê¸°ë¡ê´€ë¦¬'
        elif 'í™ë³´' in filename_lower:
            subdomain = 'í™ë³´ê´€ë¦¬'
    
    # ê²½ì˜ê´€ë¦¬ ë„ë©”ì¸
    elif any(keyword in filename_lower for keyword in ['ê²½ì˜', 'ì„±ê³¼', 'ë‚´ë¶€í†µì œ', 'ì´í•´ì¶©ëŒ', 'ì¡°ì§', 'ì§ì œ']):
        domain = 'ê²½ì˜ê´€ë¦¬'
        if 'ê²½ì˜' in filename_lower:
            subdomain = 'ê²½ì˜ì •ì±…'
        elif 'ì„±ê³¼' in filename_lower:
            subdomain = 'ì„±ê³¼ê´€ë¦¬'
        elif 'ë‚´ë¶€í†µì œ' in filename_lower:
            subdomain = 'ë‚´ë¶€í†µì œ'
        elif 'ì´í•´ì¶©ëŒ' in filename_lower:
            subdomain = 'ì´í•´ì¶©ëŒë°©ì§€'
        elif 'ì¡°ì§' in filename_lower or 'ì§ì œ' in filename_lower:
            subdomain = 'ì¡°ì§ê´€ë¦¬'
    
    # 3ì°¨ ë¶„ë¥˜: ìµœì‹ ì„± (ë“±ë¡ì¼ì ì¶”ì¶œ)
    try:
        # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ (ì˜ˆ: 240715, 221222)
        date_match = re.search(r'(\d{6})', filename)
        if date_match:
            date_str = date_match.group(1)
            year = int(date_str[:2])
            if year >= 24:  # 2024ë…„
                recency = 'ìµœì‹ '
                recency_score = 3
            elif year >= 22:  # 2022-2023ë…„
                recency = 'ìµœê·¼'
                recency_score = 2
            else:  # 2021ë…„ ì´ì „
                recency = 'ê¸°ì¡´'
                recency_score = 1
        else:
            recency = 'ê¸°ì¡´'
            recency_score = 1
    except:
        recency = 'ê¸°ì¡´'
        recency_score = 1
    
    return {
        'document_level': document_level,
        'level_description': level_description,
        'domain': domain,
        'subdomain': subdomain,
        'recency': recency,
        'recency_score': recency_score,
        'filename': filename
    }

def _smart_search(query: str, top_k: int = 10) -> List[Dict]:
    """ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ (ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì •í™•í•œ í•„í„°ë§)"""
    client = _get_qdrant_client()
    
    # ì§ˆë¬¸ì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
    query_lower = query.lower()
    target_domain = None
    
    if any(keyword in query_lower for keyword in ['ì¸ì‚¬', 'ì±„ìš©', 'ê¸‰ì—¬', 'íœ´ê°€', 'ìœ¡ì•„íœ´ì§', 'ì—°ì°¨', 'ì¶œì¥']):
        target_domain = 'ì¸ì‚¬ê´€ë¦¬'
    elif any(keyword in query_lower for keyword in ['íšŒê³„', 'ì˜ˆì‚°', 'ê°ì‚¬', 'ì¬ì •', 'ê³„ì•½', 'ìˆ˜ìˆ˜ë£Œ']):
        target_domain = 'ì¬ë¬´ê´€ë¦¬'
    elif any(keyword in query_lower for keyword in ['ë³´ì•ˆ', 'ì •ë³´ë³´í˜¸', 'ê°œì¸ì •ë³´', 'ë³´ì•ˆì—…ë¬´']):
        target_domain = 'ë³´ì•ˆê´€ë¦¬'
    elif any(keyword in query_lower for keyword in ['ê¸°ìˆ ', 'IT', 'ì •ë³´í™”', 'ì‹œìŠ¤í…œ', 'ì „ìì„œëª…']):
        target_domain = 'ê¸°ìˆ ê´€ë¦¬'
    elif any(keyword in query_lower for keyword in ['ë¬¸ì„œ', 'ê¸°ë¡', 'ìë£Œ', 'ê´€ë¦¬']):
        target_domain = 'í–‰ì •ê´€ë¦¬'
    
    try:
        if target_domain:
            # ë„ë©”ì¸ ê¸°ë°˜ í•„í„°ë§ (íŒŒì¼ëª… íŒ¨í„´ìœ¼ë¡œ)
            domain_keywords = {
                'ì¸ì‚¬ê´€ë¦¬': ['ì¸ì‚¬', 'ê¸‰ì—¬', 'ì·¨ì—…', 'ì¶œì¥', 'íœ´ê°€'],
                'ì¬ë¬´ê´€ë¦¬': ['íšŒê³„', 'ì˜ˆì‚°', 'ê°ì‚¬', 'ê³„ì•½', 'ìˆ˜ìˆ˜ë£Œ'],
                'ë³´ì•ˆê´€ë¦¬': ['ë³´ì•ˆ', 'ì •ë³´ë³´í˜¸', 'ê°œì¸ì •ë³´'],
                'ê¸°ìˆ ê´€ë¦¬': ['ê¸°ìˆ ', 'IT', 'ì •ë³´í™”', 'ì „ìì„œëª…'],
                'í–‰ì •ê´€ë¦¬': ['ë¬¸ì„œ', 'ê¸°ë¡', 'ìë£Œ', 'ê´€ë¦¬']
            }
            
            keywords = domain_keywords.get(target_domain, [])
            if keywords:
                # ì—¬ëŸ¬ í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ëœ ë¬¸ì„œ ê²€ìƒ‰
                filter_conditions = []
                for keyword in keywords:
                    filter_conditions.append({
                        "key": "source",
                        "match": {"text": keyword}
                    })
                
                results = client.scroll(
                    collection_name=settings.QDRANT_COLLECTION_NAME,
                    scroll_filter={
                        "should": filter_conditions  # OR ì¡°ê±´ìœ¼ë¡œ ê²€ìƒ‰
                    },
                    limit=top_k
                )[0]
                return results
        
        # ë„ë©”ì¸ì„ íŠ¹ì •í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ë¹ˆ ê²°ê³¼ ë°˜í™˜
        return []
        
    except Exception as e:
        print(f"ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

def _keyword_search(query: str, top_k: int = 10) -> List[Dict]:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰"""
    keywords = _extract_keywords(query)
    if not keywords:
        return []
    
    client = _get_qdrant_client()
    
    # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œ ê²€ìƒ‰
    keyword_results = []
    for keyword in keywords:
        try:
            # payloadì—ì„œ text í•„ë“œì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œ ê²€ìƒ‰
            results = client.scroll(
                collection_name=settings.QDRANT_COLLECTION_NAME,
                scroll_filter={
                    "must": [
                        {
                            "key": "text",
                            "match": {"text": keyword}
                        }
                    ]
                },
                limit=top_k // len(keywords)
            )[0]
            keyword_results.extend(results)
        except Exception as e:
            print(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì˜¤ë¥˜ ({keyword}): {e}")
            continue
    
    return keyword_results

def _vector_search(query: str, top_k: int = 10) -> List[Dict]:
    """ë²¡í„° ê¸°ë°˜ ê²€ìƒ‰"""
    client = _get_qdrant_client()
    embedder = _get_embedder()
    qvec = embedder.encode([query])[0].tolist()

    try:
        results = client.search(
            collection_name=settings.QDRANT_COLLECTION_NAME,
            query_vector=qvec,
            limit=top_k,
        )
        return results
    except Exception as e:
        print(f"ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

def _rerank_results(vector_results: List[Dict], keyword_results: List[Dict], query: str) -> List[Dict]:
    """ê²€ìƒ‰ ê²°ê³¼ ì¬ìˆœìœ„í™”"""
    all_results = {}
    
    # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
    for i, result in enumerate(vector_results):
        doc_id = result.payload.get("path", "") + "_" + str(result.payload.get("page", ""))
        if doc_id not in all_results:
            all_results[doc_id] = {
                "result": result,
                "vector_score": result.score,
                "keyword_score": 0,
                "combined_score": result.score
            }
    
    # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
    for result in keyword_results:
        doc_id = result.payload.get("path", "") + "_" + str(result.payload.get("page", ""))
        if doc_id in all_results:
            all_results[doc_id]["keyword_score"] = 1.0
            all_results[doc_id]["combined_score"] += 1.0
        else:
            all_results[doc_id] = {
                "result": result,
                "vector_score": 0,
                "keyword_score": 1.0,
                "combined_score": 1.0
            }
    
    # í†µí•© ì ìˆ˜ë¡œ ì¬ì •ë ¬
    reranked = sorted(all_results.values(), key=lambda x: x["combined_score"], reverse=True)
    return [item["result"] for item in reranked]

def _estimate_tokens(text: str) -> int:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì •"""
    # í•œêµ­ì–´ëŠ” ë³´í†µ ë‹¨ì–´ë‹¹ 1.3 í† í° ì •ë„
    words = text.split()
    return int(len(words) * 1.3)

def _optimize_context(documents: List[Dict], max_tokens: int = 4000, query: str = "") -> List[Dict]:
    """ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ìµœì í™” (ì§ˆë¬¸ ê¸°ë°˜ ìš°ì„ ìˆœìœ„)"""
    if not documents:
        return []
    
    # ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
    scored_docs = []
    for doc in documents:
        text = doc.payload.get("text", "")
        relevance_score = 0
        
        # ì§ˆë¬¸ í‚¤ì›Œë“œê°€ ë¬¸ì„œì— í¬í•¨ëœ ì •ë„ë¡œ ì ìˆ˜ ê³„ì‚°
        if query:
            query_keywords = _extract_keywords(query)
            for keyword in query_keywords:
                if keyword in text:
                    relevance_score += 1
        
        # ë²¡í„° ì ìˆ˜ë„ ê³ ë ¤
        if hasattr(doc, 'score'):
            relevance_score += doc.score * 0.5
        
        scored_docs.append({
            'doc': doc,
            'relevance_score': relevance_score,
            'estimated_tokens': _estimate_tokens(text)
        })
    
    # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
    scored_docs.sort(key=lambda x: x['relevance_score'], reverse=True)
    
    # í† í° ì œí•œ ë‚´ì—ì„œ ìµœì ì˜ ë¬¸ì„œ ì„ íƒ
    optimized_docs = []
    current_tokens = 0
    
    for scored_doc in scored_docs:
        doc_tokens = scored_doc['estimated_tokens']
        
        if current_tokens + doc_tokens <= max_tokens:
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸´ ë¬¸ì„œëŠ” ìë¥´ê¸°)
            doc = scored_doc['doc']
            text = doc.payload.get("text", "")
            if len(text) > 800:  # 800ì ì œí•œ
                doc.payload["text"] = text[:800] + "..."
            
            optimized_docs.append(doc)
            current_tokens += doc_tokens
        else:
            break
    
    return optimized_docs

def _build_context(chunks: List[Dict]) -> str:
    """ê°œì„ ëœ ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶•"""
    if not chunks:
        return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    lines = []
    for i, c in enumerate(chunks, start=1):
        src = c.payload.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")
        page = c.payload.get("page", "ì•Œ ìˆ˜ ì—†ìŒ")
        text = c.payload.get("text", "ë‚´ìš© ì—†ìŒ")
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ìë¥´ê¸°)
        if len(text) > 500:
            text = text[:500] + "..."
        
        lines.append(f"[{i}] ({src} p.{page}) {text}")
    
    return "\n\n".join(lines)

def _enhance_search_with_domain_classification(query: str, documents: List[Dict]) -> List[Dict]:
    """ë„ë©”ì¸ ë¶„ë¥˜ë¥¼ í™œìš©í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í–¥ìƒ"""
    if not documents:
        return documents
    
    enhanced_docs = []
    
    for doc in documents:
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ë¬¸ì„œ ë¶„ë¥˜ ì •ë³´ ì¶”ì¶œ
        source = doc.payload.get("source", "")
        doc_title = doc.payload.get("doc_title", "")
        
        # íŒŒì¼ëª… ê¸°ë°˜ ë„ë©”ì¸ ë¶„ë¥˜
        domain_classification = _classify_document_by_domain(source)
        
        # ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        relevance_score = 0
        
        # 1. ë„ë©”ì¸ ì¼ì¹˜ ì ìˆ˜ (ë†’ì€ ê°€ì¤‘ì¹˜)
        query_lower = query.lower()
        
        # ì¸ì‚¬ ê´€ë ¨ ì§ˆë¬¸
        if any(keyword in query_lower for keyword in ['ì¸ì‚¬', 'ì±„ìš©', 'ê¸‰ì—¬', 'íœ´ê°€', 'ìœ¡ì•„íœ´ì§', 'ì—°ì°¨', 'ì¶œì¥']):
            if domain_classification['domain'] == 'ì¸ì‚¬ê´€ë¦¬':
                relevance_score += 5
                if any(keyword in query_lower for keyword in ['ê¸‰ì—¬', 'ìˆ˜ë‹¹']):
                    if domain_classification['subdomain'] == 'ê¸‰ì—¬ê´€ë¦¬':
                        relevance_score += 3
                elif any(keyword in query_lower for keyword in ['ìœ¡ì•„íœ´ì§', 'íœ´ê°€']):
                    if domain_classification['subdomain'] in ['ì¸ì‚¬ì •ì±…', 'ë³µë¬´ê´€ë¦¬']:
                        relevance_score += 3
        
        # ì¬ë¬´ ê´€ë ¨ ì§ˆë¬¸
        elif any(keyword in query_lower for keyword in ['íšŒê³„', 'ì˜ˆì‚°', 'ê°ì‚¬', 'ì¬ì •', 'ê³„ì•½', 'ìˆ˜ìˆ˜ë£Œ']):
            if domain_classification['domain'] == 'ì¬ë¬´ê´€ë¦¬':
                relevance_score += 5
                if 'ê°ì‚¬' in query_lower and domain_classification['subdomain'] == 'ê°ì‚¬ê´€ë¦¬':
                    relevance_score += 3
        
        # ë³´ì•ˆ ê´€ë ¨ ì§ˆë¬¸
        elif any(keyword in query_lower for keyword in ['ë³´ì•ˆ', 'ì •ë³´ë³´í˜¸', 'ê°œì¸ì •ë³´']):
            if domain_classification['domain'] == 'ë³´ì•ˆê´€ë¦¬':
                relevance_score += 5
        
        # ê¸°ìˆ  ê´€ë ¨ ì§ˆë¬¸
        elif any(keyword in query_lower for keyword in ['ê¸°ìˆ ', 'IT', 'ì •ë³´í™”', 'ì‹œìŠ¤í…œ']):
            if domain_classification['domain'] == 'ê¸°ìˆ ê´€ë¦¬':
                relevance_score += 5
        
        # 2. ë¬¸ì„œ ê³„ì¸µ ì ìˆ˜
        if 'ê·œì •' in query_lower or 'ê·œì¹™' in query_lower:
            if domain_classification['document_level'] in ['ê·œì •', 'ê·œì¹™']:
                relevance_score += 2
        
        # 3. ìµœì‹ ì„± ì ìˆ˜
        relevance_score += domain_classification['recency_score']
        
        # 4. ë²¡í„° ì ìˆ˜ (ê¸°ì¡´ ì ìˆ˜ ìœ ì§€)
        if hasattr(doc, 'score'):
            relevance_score += doc.score
        
        # í–¥ìƒëœ ë¬¸ì„œ ê°ì²´ ìƒì„±
        enhanced_doc = {
            'doc': doc,
            'domain_classification': domain_classification,
            'relevance_score': relevance_score
        }
        
        enhanced_docs.append(enhanced_doc)
    
    # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
    enhanced_docs.sort(key=lambda x: x['relevance_score'], reverse=True)
    
    # ì›ë³¸ ë¬¸ì„œ í˜•íƒœë¡œ ë°˜í™˜
    return [item['doc'] for item in enhanced_docs]

def hybrid_search(question: str, top_k: int = None) -> List[Dict]:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ + ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸)"""
    top_k = top_k or settings.RAG_TOP_K
    
    # ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ (ë©”íƒ€ë°ì´í„° ê¸°ë°˜)
    smart_results = _smart_search(question, top_k=top_k//2)
    
    # ë²¡í„° ê²€ìƒ‰
    vector_results = _vector_search(question, top_k=top_k)
    
    # í‚¤ì›Œë“œ ê²€ìƒ‰
    keyword_results = _keyword_search(question, top_k=top_k)
    
    # ëª¨ë“  ê²°ê³¼ í†µí•©
    all_results = smart_results + vector_results + keyword_results
    
    # ê²°ê³¼ ì¬ìˆœìœ„í™”
    combined_results = _rerank_results(vector_results, keyword_results, question)
    
    # ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš°ì„ ìˆœìœ„ë¡œ ì¶”ê°€
    if smart_results:
        # ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë§¨ ì•ì— ë°°ì¹˜
        final_results = smart_results + [r for r in combined_results if r not in smart_results]
    else:
        final_results = combined_results
    
    # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í–¥ìƒ
    enhanced_results = _enhance_search_with_domain_classification(question, final_results)
    
    # ì»¨í…ìŠ¤íŠ¸ ìµœì í™” (ì§ˆë¬¸ ê¸°ë°˜)
    optimized_results = _optimize_context(enhanced_results, max_tokens=4000, query=question)
    
    return optimized_results

def generate_answer(question: str, retrieved: List[Dict]) -> Dict:
    """ê°œì„ ëœ ë‹µë³€ ìƒì„±"""
    if not retrieved:
        return {
            "answer": "ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” ëª…í™•í•˜ê²Œ í•´ì£¼ì‹œë©´ ê°ì‚¬í•©ë‹ˆë‹¤.",
            "sources": []
        }
    
    ctx = _build_context(retrieved)
    client = OpenAI(api_key=settings.OPENAI_API_KEY)

    user_prompt = f"""ì»¨í…ìŠ¤íŠ¸:
{ctx}

ì§ˆë¬¸:
{question}

ìš”êµ¬ì‚¬í•­:
- í•œêµ­ì–´ë¡œ ë‹µë³€
- ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê·¼ê±°ê°€ ë˜ëŠ” ì¡°í•­ê³¼ ë¬¸ì„œëª…ì„ ê°„ë‹¨íˆ ëª…ì‹œ
- ì‹¤ë¬´ ì ìš© ì‹œ ì£¼ì˜ì‚¬í•­ì´ë‚˜ ì ˆì°¨ë¥¼ í¬í•¨
- ë‹µë³€ í›„ ì°¸ê³ í•œ ë¬¸ì„œëª…ê³¼ í˜ì´ì§€ë¥¼ ëª…ì‹œ
"""

    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            temperature=0.1,  # ë” ì¼ê´€ëœ ë‹µë³€ì„ ìœ„í•´ ë‚®ì¶¤
            max_tokens=1500,  # ë‹µë³€ ê¸¸ì´ ì œí•œ
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt},
            ],
        )
        answer = resp.choices[0].message.content
    except Exception as e:
        print(f"OpenAI API ì˜¤ë¥˜: {e}")
        answer = f"ì£„ì†¡í•©ë‹ˆë‹¤. AI ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    # ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ
    sources = [
        {
            "source": r.payload.get("source", "ì•Œ ìˆ˜ ì—†ìŒ"),
            "page": r.payload.get("page", "ì•Œ ìˆ˜ ì—†ìŒ"),
            "path": r.payload.get("path", "ì•Œ ìˆ˜ ì—†ìŒ"),
        }
        for r in retrieved
    ]
    
    return {"answer": answer, "sources": sources}

def _classify_question_type(query: str) -> Dict[str, str]:
    """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ì§ˆë¬¸ ë¶„ë¥˜"""
    query_lower = query.lower()
    
    # 1ë‹¨ê³„: ëª…í™•í•œ ì¸ì‚¬ë§/ê°ì‚¬ì¸ì‚¬ íŒ¨í„´ (ë¹ ë¥¸ ì²˜ë¦¬)
    clear_greeting_patterns = [
        'ì•ˆë…•í•˜ì„¸ìš”', 'ë°˜ê°‘ìŠµë‹ˆë‹¤', 'ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤', 'ì²˜ìŒ ëµ™ê² ìŠµë‹ˆë‹¤',
        'ê°ì‚¬í•©ë‹ˆë‹¤', 'ê³ ë§™ìŠµë‹ˆë‹¤', 'ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤', 'í˜ë‚´ì„¸ìš”', 'í™”ì´íŒ…'
    ]
    
    if any(pattern in query_lower for pattern in clear_greeting_patterns):
        return {
            'type': 'greeting',
            'category': 'ëª…í™•í•œ ì¸ì‚¬ë§/ê°ì‚¬ì¸ì‚¬',
            'needs_rag': False,
            'confidence': 'high',
            'response_type': 'friendly_greeting'
        }
    
    # 2ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê·œì • ê´€ë ¨ì„± ë¶„ì„
    regulation_keywords = [
        'ê·œì •', 'ê·œì¹™', 'ì§€ì¹¨', 'ì ˆì°¨', 'ë°©ë²•', 'ì‹ ì²­', 'ì‹ ê³ ', 'ì²˜ë¦¬',
        'ìœ¡ì•„íœ´ì§', 'ì—°ì°¨', 'ê¸‰ì—¬', 'ì¶œì¥', 'íšŒê³„', 'ê°ì‚¬', 'ë³´ì•ˆ', 'ì •ë³´ë³´í˜¸',
        'ì±„ìš©', 'ì¸ì‚¬', 'ê³„ì•½', 'ìˆ˜ìˆ˜ë£Œ', 'ë™í˜¸íšŒ', 'êµìœ¡', 'í›ˆë ¨'
    ]
    
    regulation_score = sum(1 for keyword in regulation_keywords if keyword in query_lower)
    
    # 3ë‹¨ê³„: ì§ˆë¬¸ ì˜ë„ ë¶„ì„ (ì˜ë¬¸ì‚¬, ëª…ë ¹ì–´ ë“±)
    question_indicators = ['ì–´ë–»ê²Œ', 'ë¬´ì—‡', 'ì–¸ì œ', 'ì–´ë””ì„œ', 'ì™œ', 'ì–´ë–¤', 'ëª‡', 'ì–¼ë§ˆë‚˜']
    has_question_intent = any(indicator in query_lower for indicator in question_indicators)
    
    # 4ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ë¶„ë¥˜ ê²°ì •
    if regulation_score >= 2 and has_question_intent:
        # ëª…í™•í•œ ê·œì • ì§ˆë¬¸
        return {
            'type': 'regulation',
            'category': 'ê·œì •/ì œë„ ì§ˆë¬¸',
            'needs_rag': True,
            'confidence': 'high',
            'response_type': 'regulation_answer',
            'regulation_score': regulation_score
        }
    elif regulation_score >= 1 and has_question_intent:
        # ê·œì • ê´€ë ¨ ì§ˆë¬¸ ê°€ëŠ¥ì„±
        return {
            'type': 'regulation',
            'category': 'ê·œì • ê´€ë ¨ ì§ˆë¬¸',
            'needs_rag': True,
            'confidence': 'medium',
            'response_type': 'regulation_answer',
            'regulation_score': regulation_score
        }
    elif regulation_score == 0 and not has_question_intent:
        # ì¸ì‚¬ë§/ì¼ë°˜ ëŒ€í™”
        return {
            'type': 'greeting',
            'category': 'ì¼ë°˜ ì¸ì‚¬ë§/ëŒ€í™”',
            'needs_rag': False,
            'confidence': 'medium',
            'response_type': 'friendly_greeting'
        }
    else:
        # ëª¨í˜¸í•œ ê²½ìš° - RAG ê²€ìƒ‰ ì‹œë„
        return {
            'type': 'ambiguous',
            'category': 'ëª¨í˜¸í•œ ì§ˆë¬¸',
            'needs_rag': True,
            'confidence': 'low',
            'response_type': 'general_answer',
            'regulation_score': regulation_score
        }

def _generate_smart_greeting_response(query: str, question_type: Dict) -> str:
    """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì¸ì‚¬ë§ ì‘ë‹µ ìƒì„±"""
    query_lower = query.lower()
    
    # ê¸°ë³¸ ì¸ì‚¬ë§
    base_greeting = "ì•ˆë…•í•˜ì„¸ìš”! í•œêµ­ì¸í„°ë„·ì§„í¥ì› ê·œì •ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."
    
    # ì‹ ì…ì‚¬ì› ê´€ë ¨ íŠ¹ë³„ ì‘ë‹µ
    if 'ì‹ ì…' in query_lower:
        return f"{base_greeting}\n\nì‹ ì…ì‚¬ì›ì´ì‹œêµ°ìš”! í™˜ì˜í•©ë‹ˆë‹¤! ğŸ˜Š\n\nê¶ê¸ˆí•œ ê·œì •ì´ë‚˜ ì œë„ê°€ ìˆìœ¼ì‹œë©´ êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´:\nâ€¢ ìœ¡ì•„íœ´ì§ ì‹ ì²­ ì ˆì°¨\nâ€¢ ì—°ì°¨ ì‚¬ìš© ë°©ë²•\nâ€¢ ê¸‰ì—¬ ì§€ê¸‰ ê·œì •\nâ€¢ ì¶œì¥ ì‹ ì²­ ì ˆì°¨\n\nì–´ë–¤ ê²ƒì´ë“  ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤!"
    
    # ê°ì‚¬ì¸ì‚¬
    elif any(word in query_lower for word in ['ê°ì‚¬', 'ê³ ë§™']):
        return "ì²œë§Œì—ìš”! ì–¸ì œë“ ì§€ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ğŸ˜Š"
    
    # ì¼ë°˜ ì¸ì‚¬ë§
    else:
        return base_greeting

def _should_use_rag(query: str, question_type: Dict) -> bool:
    """RAG ì‚¬ìš© ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” ìŠ¤ë§ˆíŠ¸ ë¡œì§"""
    
    # 1. ëª…í™•í•œ ê·œì • ì§ˆë¬¸
    if question_type['type'] == 'regulation' and question_type['confidence'] == 'high':
        return True
    
    # 2. ëª…í™•í•œ ì¸ì‚¬ë§
    if question_type['type'] == 'greeting' and question_type['confidence'] == 'high':
        return False
    
    # 3. ëª¨í˜¸í•œ ê²½ìš° - ì¶”ê°€ ë¶„ì„
    if question_type['confidence'] == 'low':
        # ì§ˆë¬¸ ê¸¸ì´ì™€ ë³µì¡ì„± ë¶„ì„
        query_length = len(query.strip())
        if query_length < 10:  # ë„ˆë¬´ ì§§ì€ ì§ˆë¬¸
            return False
        
        # ê·œì • ê´€ë ¨ í‚¤ì›Œë“œ ë°€ë„ ê³„ì‚°
        regulation_keywords = ['ê·œì •', 'ê·œì¹™', 'ì§€ì¹¨', 'ì ˆì°¨', 'ë°©ë²•', 'ì‹ ì²­', 'ì‹ ê³ ', 'ì²˜ë¦¬']
        keyword_density = sum(1 for keyword in regulation_keywords if keyword in query.lower()) / len(query.split())
        
        if keyword_density > 0.1:  # í‚¤ì›Œë“œ ë°€ë„ê°€ ë†’ìœ¼ë©´ RAG ì‚¬ìš©
            return True
    
    # 4. ê¸°ë³¸ê°’
    return question_type['needs_rag']

def rag_answer(question: str) -> Dict:
    """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ RAG ë‹µë³€ ìƒì„±"""
    print(f"DEBUG: RAG ì‹œìŠ¤í…œ ì‹œì‘ - ì§ˆë¬¸: {question}")
    
    # 1ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì§ˆë¬¸ ë¶„ë¥˜
    question_type = _classify_question_type(question)
    print(f"DEBUG: ì§ˆë¬¸ ë¶„ë¥˜: {question_type['type']} - {question_type['category']} (ì‹ ë¢°ë„: {question_type['confidence']})")
    
    # 2ë‹¨ê³„: ìŠ¤ë§ˆíŠ¸ RAG ì‚¬ìš© ì—¬ë¶€ ê²°ì •
    use_rag = _should_use_rag(question, question_type)
    print(f"DEBUG: RAG ì‚¬ìš© ì—¬ë¶€: {use_rag}")
    
    # 3ë‹¨ê³„: RAG ê²€ìƒ‰ì´ ë¶ˆí•„ìš”í•œ ê²½ìš°
    if not use_rag:
        print(f"DEBUG: RAG ê²€ìƒ‰ ìƒëµ - {question_type['response_type']} ì‘ë‹µ ìƒì„±")
        response = _generate_smart_greeting_response(question, question_type)
        return {
            "answer": response,
            "sources": [],
            "question_type": question_type,
            "rag_used": False
        }
    
    # 4ë‹¨ê³„: RAG ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°
    try:
        print(f"DEBUG: RAG ê²€ìƒ‰ ì‹œì‘ - {question_type['response_type']}")
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        retrieved = hybrid_search(question)
        print(f"DEBUG: ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved)}")
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•œ ê²½ìš° (3ê°œ ë¯¸ë§Œ)
        if len(retrieved) < 3:
            print(f"DEBUG: ê²€ìƒ‰ ê²°ê³¼ ë¶€ì¡± - ì‚¬ìš©ì ì¹œí™”ì  ë©”ì‹œì§€ ìƒì„±")
            return {
                "answer": "ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” ëª…í™•í•˜ê²Œ í•´ì£¼ì‹œë©´ ê°ì‚¬í•©ë‹ˆë‹¤.",
                "sources": [],
                "question_type": question_type,
                "rag_used": False
            }
        
        # ë‹µë³€ ìƒì„±
        result = generate_answer(question, retrieved)
        print(f"DEBUG: RAG ì‹œìŠ¤í…œ ì™„ë£Œ")
        
        # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶”ê°€
        result['question_type'] = question_type
        result['rag_used'] = True
        
        return result
    except Exception as e:
        print(f"DEBUG: RAG ì‹œìŠ¤í…œ ì˜¤ë¥˜: {str(e)}")
        return {
            "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. RAG ì‹œìŠ¤í…œì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "sources": [],
            "question_type": question_type,
            "rag_used": False
        }

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
def retrieve(question: str, top_k: int = None) -> List[Dict]:
    """ê¸°ì¡´ retrieve í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    return hybrid_search(question, top_k)

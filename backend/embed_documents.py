#!/usr/bin/env python3
"""
PDF â†’ Clean Text â†’ Page-aware Chunking â†’ KoE5 Embedding â†’ Qdrant Upsert
- ë¶ˆí•„ìš” ë¬¸ì/ì œì–´ë¬¸ì ì œê±°(clean_text)
- í˜ì´ì§€ ë²ˆí˜¸ ë³´ì¡´ + ì•ˆì •ì  point ID(uuid5)
- í™•ì¥ëœ ë©”íƒ€ë°ì´í„°(payload = metadata)
- ìì£¼ ì“°ëŠ” í•„ë“œ ì¸ë±ìŠ¤ ìƒì„±

ì‚¬ìš©ë²•:
  python embed_documents.py                    # ê¸°ì¡´ ë°ì´í„° ìœ ì§€ (ê¸°ë³¸ê°’)
  python embed_documents.py --reset           # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆë¡œ ì‹œì‘
  python embed_documents.py -r                # --resetì˜ ì¶•ì•½í˜•

ì‚¬ìš© ì „ í™˜ê²½ë³€ìˆ˜(.env í˜¹ì€ ì‹œìŠ¤í…œ í™˜ê²½):
  PDF_DIR=/app/documents/kisa_pdf
  QDRANT_HOST=qdrant
  QDRANT_PORT=6333
  COLLECTION_NAME=regulations_final
  RESET_COLLECTION=false
  BATCH_SIZE=256

í•„ìš” íŒ¨í‚¤ì§€:
  pip install qdrant-client sentence-transformers PyPDF2 tqdm python-dotenv
"""
from __future__ import annotations

import os
import re
import sys
import uuid
import argparse
from uuid import uuid5, NAMESPACE_URL
from pathlib import Path
from typing import Dict, Any, List, Tuple
from datetime import datetime, timezone, timedelta

from dotenv import load_dotenv
from tqdm import tqdm
from PyPDF2 import PdfReader

from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance, PointStruct

# -------------------- í™˜ê²½ --------------------
load_dotenv()
PDF_DIR = os.getenv("PDF_DIR", "/app/documents/kisa_pdf")
FORMS_DIR = os.getenv("FORMS_DIR", "/app/documents/kisa_pdf/forms_extracted_v6")
QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "regulations_final")
RESET_COLLECTION = os.getenv("RESET_COLLECTION", "false").lower() == "true"
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "256"))

# Embedding: KoE5 (í•œêµ­ì–´ ìµœì í™”, 1024ì°¨ì›)
EMBED_MODEL = os.getenv("HF_MODEL", "nlpai-lab/KoE5")
EMBED_BACKEND = "hf"
EMBED_DIM = 1024

KST = timezone(timedelta(hours=9))

# -------------------- ì„œì‹ ê´€ë ¨ ìƒìˆ˜ --------------------
# ì„œì‹ ì‹œì‘ íŒ¨í„´ (pdf_form_extractor_v6.pyì—ì„œ ê°€ì ¸ì˜´)
FORM_START_PATTERNS = [
    re.compile(r'^\s*\[ë³„ì§€\s*ì œ\d+í˜¸\s*ì„œì‹\]'),
    re.compile(r'^\s*\[ë³„í‘œ\s*\d*\]'),
    re.compile(r'^\s*\[ë¶€ë¡\s*\d*\]'),
    re.compile(r'^\s*\[ì²¨ë¶€ì„œì‹\s*\d*\]'),
    re.compile(r'^\s*\[ì²¨ë¶€ì–‘ì‹\s*\d*\]'),
]

# ì„œì‹ ì œëª© íŒ¨í„´ (pdf_form_extractor_v6.pyì—ì„œ ê°€ì ¸ì˜´)
FORM_TITLE_PATTERNS = [
    # ì‹ ì²­/ì œì¶œ ê´€ë ¨
    r'ì‹ ì²­ì„œ', r'ì œì¶œì„œ', r'ì²­êµ¬ì„œ', r'ìš”ì²­ì„œ', r'ìš”êµ¬ì„œ', r'ì‚¬ìœ ì„œ', r'ì·¨ì†Œì‹ ì²­ì„œ', r'ì¬ë°œê¸‰ì‹ ì²­ì„œ', r'ì¸ì¦ì—°ì¥ì‹ ì²­ì„œ',
    # ë³´ê³ /í‰ê°€ ê´€ë ¨
    r'ë³´ê³ ì„œ', r'í‰ê°€ì„œ', r'í‰ê°€í‘œ', r'ì ê²€í‘œ', r'ì²´í¬ë¦¬ìŠ¤íŠ¸', r'í™•ì¸ì„œ', r'ìš”ì•½ì„œ', r'ê²°ê³¼í‘œ', r'ê²€í† ì„œ', r'ê²°ê³¼í™•ì¸ì„œ', r'ì™„ë£Œí™•ì¸ì„œ',
    # ì„œì•½/ê³„ì•½ ê´€ë ¨
    r'ì„œì•½ì„œ', r'ê³„ì•½ì„œ', r'í˜‘ì•½ì„œ', r'ê°ì„œ', r'ì¡°ì„œ', r'í™•ì•½ì„œ', r'ìœ¤ë¦¬ì„œì•½ì„œ', r'ë³´ì•ˆì„œì•½ì„œ', r'ì§ë¬´ìœ¤ë¦¬ì„œì•½ì„œ',
    # ìŠ¹ì¸/í†µì§€ ê´€ë ¨
    r'ìŠ¹ì¸ì„œ', r'í†µì§€ì„œ', r'í†µë³´ì„œ', r'ë°œì£¼ì„œ', r'ì…ì°°ì„œ', r'ê³ ì§€', r'ì²˜ë¶„ì„œ',
    # ë“±ë¡/ë³€ê²½ ê´€ë ¨
    r'ë“±ë¡ì„œ', r'ë³€ê²½ì„œ', r'íì§€ì„œ', r'ì •ì§€ì„œ', r'íšŒë³µì„œ', r'ì‘ì—…ì¤‘ì§€',
    # ê´€ë¦¬/ìš´ì˜ ê´€ë ¨
    r'ê´€ë¦¬ì„œ', r'ìš´ì˜ì„œ', r'ì²˜ë¦¬ì„œ', r'ê´€ë¦¬ë°©ì¹¨', r'ì „ê²°ê·œì •', r'ì²˜ë¦¬ë‚´ì—­', r'ê´€ë¦¬ëŒ€ì¥', r'ì¶œì…ê´€ë¦¬ëŒ€ì¥',
    # ëŒ€ì¥/ì ‘ìˆ˜ ê´€ë ¨
    r'ëŒ€ì¥', r'ì ‘ìˆ˜ì¦', r'ì ‘ìˆ˜ëŒ€ì¥', r'ìœ„ì´‰ì¥', r'ì¼ì§€', r'ì ê²€ì¼ì§€', r'ì´ë ¥ì¹´ë“œ', r'í‚¤ê´€ë¦¬ëŒ€ì¥', r'ë¬¸ì„œê´€ë¦¬ëŒ€ì¥',
    # ì¸ì‚¬ ê´€ë ¨
    r'ìê²©ê¸°ì¤€', r'ì§€ê¸‰ê¸°ì¤€', r'ì „í˜•ë‹¨ê³„', r'ì‚¬ì§ì›', r'íœ´ì§ì›', r'ë³µì§ì›', r'ì¶”ì²œì„œ', r'ì¸ì ì‚¬í•­', r'ì¶”ì²œì‚¬ìœ ', r'ì‹¬ì‚¬', r'ì„±ê³¼í‰ê°€í‘œ',
    # ê¸‰ì—¬ ê´€ë ¨
    r'ê¸‰ì—¬', r'ì‚°ì •ê¸°ì¤€ì•¡', r'ìˆ˜ë‹¹', r'ëª…ì˜ˆí‡´ì§ê¸ˆ', r'ì§ë¬´ê¸‰', r'ìê²©ìˆ˜ë‹¹ì§€ê¸‰ì‹ ì²­ì„œ',
    # ì•ˆì „ë³´ê±´ ê´€ë ¨
    r'ì•ˆì „ë³´ê±´ê´€ë¦¬ì²´ê³„', r'ì‚°ì—…ì•ˆì „ë³´ê±´ìœ„ì›íšŒ',
    # ì¡°ì§/ì§ë¬´ ê´€ë ¨
    r'ì¡°ì§ë„', r'ì§ë¬´ë¶„ì¥í‘œ', r'ë¶„ë¥˜í‘œ', r'ì§í˜¸', r'ì§ë¬´ëª…', r'ì§ë¬´êµ¬ë¶„',
    # ì¸ì¦/ì‹œí—˜ ê´€ë ¨
    r'ì¸ì¦ì„œ', r'ì‹œí—˜ì‹ ì²­ì„œ', r'ì‹œí—˜ê³„ì•½ì„œ', r'ì‹œí—˜ê²°ê³¼ì„œ', r'ì¸ì¦ë§ˆí¬', r'í™•ì¸ë§ˆí¬',
    # ê¸°íƒ€
    r'ëª…ì„¸ì„œ', r'ë‚´ì—­ì„œ', r'í˜„í™©', r'í”„ë¡œíŒŒì¼', r'ë™ì˜ì„œ', r'ìš”ì•½ì„œ', r'ê²°ê³¼ì„œ', r'ì˜ê²¬ì„œ', r'í†µì§€ì„œ', r'ì²˜ë¦¬ì„œ', r'ê´€ë¦¬ì„œ', r'ìš´ì˜ì„œ', r'ì²˜ë¦¬ë‚´ì—­', r'ê´€ë¦¬ë°©ì¹¨', r'ì „ê²°ê·œì •', r'ì²˜ë¦¬ë‚´ì—­', r'ë™ì˜ì„œ', r'ìš”ì²­ì„œ', r'ì‚¬ìœ ì„œ', r'ì²˜ë¶„ì„œ', r'ì¡°ì„œ', r'ì¼ì§€', r'í˜„í™©', r'ëª…ì„¸ì„œ', r'ë‚´ì—­ì„œ', r'ì™„ë£Œí™•ì¸ì„œ', r'ì·¨ì†Œì‹ ì²­ì„œ'
]

# ë™ì˜ì–´ ì‚¬ì „ (ë„ë©”ì¸ë³„)
SYNONYM_DICT = {
    'í‡´ì§': ['í‡´ì‚¬', 'ì‚¬ì§', 'í‡´ì§ì›', 'ì‚¬ì§ì›'],
    'íœ´ì§': ['íœ´ê°€', 'íœ´ì§ì›'],
    'ë³µì§': ['ë³µê·€', 'ë³µì§ì›'],
    'ê¸‰ì—¬': ['ë´‰ê¸‰', 'ì„ê¸ˆ', 'ê¸‰ë£Œ'],
    'ì±„ìš©': ['ì„ìš©', 'ê³ ìš©', 'ì±„ìš©ì›'],
    'êµìœ¡': ['í›ˆë ¨', 'ì—°ìˆ˜', 'êµìœ¡í›ˆë ¨'],
    'ë³´ì•ˆ': ['ì •ë³´ë³´í˜¸', 'ì •ë³´ë³´ì•ˆ', 'ë³´ì•ˆê´€ë¦¬'],
    'ê°œì¸ì •ë³´': ['ê°œì¸ì •ë³´ë³´í˜¸', 'ê°œì¸ì •ë³´ê´€ë¦¬'],
    'ë¯¼ì›': ['ì‹ ê³ ', 'ë¯¼ì›ì²˜ë¦¬'],
    'íšŒê³„': ['íšŒê³„ê´€ë¦¬', 'ì¥ë¶€ê´€ë¦¬'],
    'ê°ì‚¬': ['ê°ì‚¬ê´€ë¦¬', 'ê°ì‚¬ì¸'],
    'ê³„ì•½': ['ê³„ì•½ê´€ë¦¬', 'ê³„ì•½ì‚¬ë¬´'],
    'ìì‚°': ['ìì‚°ê´€ë¦¬', 'ë¹„ìœ ë™ìì‚°'],
    'ì •ë³´í™”': ['ì •ë³´ì‹œìŠ¤í…œ', 'ì •ë³´í™”ê´€ë¦¬'],
    'ì „ìì„œëª…': ['ì¸ì¦', 'ì „ìì„œëª…ê´€ë¦¬'],
    'ë¬¸ì„œ': ['ë¬¸ì„œê´€ë¦¬', 'ìë£Œê´€ë¦¬'],
    'ê¸°ë¡': ['ê¸°ë¡ë¬¼', 'ê¸°ë¡ê´€ë¦¬'],
    'ì„±ê³¼': ['ì„±ê³¼í‰ê°€', 'ì„±ê³¼ê´€ë¦¬'],
    'ë‚´ë¶€í†µì œ': ['í†µì œ', 'ë‚´ë¶€í†µì œê´€ë¦¬'],
    'ì¡°ì§': ['ì¡°ì§ê´€ë¦¬', 'ì§ì œê´€ë¦¬']
}

# -------------------- ìœ í‹¸ --------------------
def now_year_kst() -> int:
    return datetime.now(KST).year

def calculate_recency_score(year: int) -> int:
    if not year:
        return 1
    cy = now_year_kst()
    if year >= cy:
        return 3
    elif year >= cy - 2:
        return 2
    else:
        return 1

def parse_register_date_from_filename(filename: str) -> Dict[str, Any]:
    """íŒŒì¼ëª… ë‚´ (YYMMDD) â†’ ISO ë° íŒŒìƒê°’
    ì˜ˆ) 3_16_ì „ìì„œëª…... (210809).pdf â†’ 2021-08-09
    """
    m = re.search(r"\((\d{6})\)", filename)
    if not m:
        return {"register_date_iso": None, "year": 0, "month": 0, "day": 0}
    y, mo, d = m.group(1)[:2], m.group(1)[2:4], m.group(1)[4:6]
    year = 2000 + int(y)
    return {
        "register_date_iso": f"{year:04d}-{int(mo):02d}-{int(d):02d}",
        "year": year,
        "month": int(mo),
        "day": int(d)
    }

# ë¶ˆí•„ìš” ë¬¸ì/ì œì–´ë¬¸ì ì œê±°
_DEF_SYMBOLS = "â–¡â– â—‹â—â—†â—‡â–¶â–·â—€â—â€»â˜†â˜…â€¢ã†âˆ™â—¦â—â–ªï¸â–«ï¸â–âœ“âœ”âœ—âœ˜âŒ"

def clean_text(text: str) -> str:
    if not text:
        return ""
    # Zero-width & ì œì–´ë¬¸ì ì œê±°
    text = re.sub(r"[\u200B-\u200D\uFEFF]", "", text)              # zero-width
    text = re.sub(r"[\x00-\x1f\x7f-\x9f]", " ", text)              # C0/C1 control
    # ë„í˜•/ê¸€ë¨¸ë¦¬ ê¸°í˜¸ ì œê±° (í•„ìš” ì‹œ ì¹˜í™˜ íƒœê¹…ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥)
    text = re.sub(f"[{re.escape(_DEF_SYMBOLS)}]", " ", text)
    # í•˜ì´í”ˆ ì¤„ë°”ê¿ˆ ì—°ê²°(ì›Œë“œë© ì•„í‹°íŒ©íŠ¸)
    text = re.sub(r"-\s*\n\s*", "", text)
    # ì—¬ëŸ¬ ê³µë°±/íƒ­ ì •ê·œí™”
    text = re.sub(r"[ \t]+", " ", text)
    # ì—°ì† ê³µë°± ì¶•ì†Œ
    text = re.sub(r"\s{2,}", " ", text)
    return text.strip()

# ë¨¸ë¦¬ë§ ì œê±° ì •ê·œì‹ ìˆ˜ì •
_DEF_HEADER_RE = re.compile(r"ì œ\s*\d+\s*(ì¥|ì¡°)")

def strip_header(text: str) -> str:
    m = _DEF_HEADER_RE.search(text or "")
    return text[m.start():] if m else (text or "")

# íŒŒì¼ëª… ê¸°ë°˜ ë„ë©”ì¸/ì„œë¸Œë„ë©”ì¸/ë¬¸ì„œíƒ€ì… ì¶”ì •

def classify_domain_by_filename(filename: str) -> str:
    f = filename.casefold()
    if any(k in f for k in ["ì¸ì‚¬","ê¸‰ì—¬","ì±„ìš©","ë³µë¬´","êµìœ¡"]):
        return "ì¸ì‚¬ê´€ë¦¬"
    if any(k in f for k in ["íšŒê³„","ì˜ˆì‚°","ê°ì‚¬","ì¬ì •","ê³„ì•½","ìì‚°"]):
        return "ì¬ë¬´ê´€ë¦¬"
    if any(k in f for k in ["ë³´ì•ˆ","ì •ë³´ë³´í˜¸","ê°œì¸ì •ë³´","ë¯¼ì›"]):
        return "ë³´ì•ˆê´€ë¦¬"
    if any(k in f for k in ["ê¸°ìˆ ","ì •ë³´í™”","ì „ìì„œëª…","ì‹œìŠ¤í…œ","it"]):
        return "ê¸°ìˆ ê´€ë¦¬"
    if any(k in f for k in ["ë¬¸ì„œ","ìë£Œ","ê¸°ë¡","í™ë³´"]):
        return "í–‰ì •ê´€ë¦¬"
    if any(k in f for k in ["ê²½ì˜","ì„±ê³¼","ë‚´ë¶€í†µì œ","ì¡°ì§","ì§ì œ"]):
        return "ê²½ì˜ê´€ë¦¬"
    return "ì¼ë°˜"

def extract_subdomain_by_filename(filename: str, domain: str) -> str:
    f = filename.casefold()
    if domain == "ì¸ì‚¬ê´€ë¦¬":
        if any(k in f for k in ["ê¸‰ì—¬","ë´‰ê¸‰"]): return "ê¸‰ì—¬ê´€ë¦¬"
        if any(k in f for k in ["ì±„ìš©","ì„ìš©"]): return "ì±„ìš©ê´€ë¦¬"
        if any(k in f for k in ["ë³µë¬´","ê·¼ë¬´"]): return "ë³µë¬´ê´€ë¦¬"
        if any(k in f for k in ["êµìœ¡","í›ˆë ¨"]): return "êµìœ¡í›ˆë ¨"
        return "ì¸ì‚¬ì •ì±…"
    if domain == "ì¬ë¬´ê´€ë¦¬":
        if any(k in f for k in ["íšŒê³„","ì¥ë¶€"]): return "íšŒê³„ê´€ë¦¬"
        if any(k in f for k in ["ê°ì‚¬","ê°ì‚¬ì¸"]): return "ê°ì‚¬ê´€ë¦¬"
        if any(k in f for k in ["ê³„ì•½","ê³„ì•½ì‚¬ë¬´"]): return "ê³„ì•½ê´€ë¦¬"
        if any(k in f for k in ["ìì‚°","ë¹„ìœ ë™ìì‚°"]): return "ìì‚°ê´€ë¦¬"
        return "ì¬ë¬´ì •ì±…"
    if domain == "ë³´ì•ˆê´€ë¦¬":
        if any(k in f for k in ["ì •ë³´ë³´í˜¸","ì •ë³´ë³´ì•ˆ"]): return "ì •ë³´ë³´í˜¸"
        if any(k in f for k in ["ê°œì¸ì •ë³´","ê°œì¸ì •ë³´ë³´í˜¸"]): return "ê°œì¸ì •ë³´ë³´í˜¸"
        if any(k in f for k in ["ë¯¼ì›","ì‹ ê³ "]): return "ë¯¼ì›ì‹ ê³ "
        return "ë³´ì•ˆì •ì±…"
    if domain == "ê¸°ìˆ ê´€ë¦¬":
        if any(k in f for k in ["ì •ë³´í™”","ì •ë³´ì‹œìŠ¤í…œ"]): return "ì •ë³´í™”ê´€ë¦¬"
        if any(k in f for k in ["ì „ìì„œëª…","ì¸ì¦"]): return "ì „ìì„œëª…ê´€ë¦¬"
        return "ê¸°ìˆ ì •ì±…"
    if domain == "í–‰ì •ê´€ë¦¬":
        if any(k in f for k in ["ë¬¸ì„œ","ë¬¸ì„œê´€ë¦¬"]): return "ë¬¸ì„œê´€ë¦¬"
        if any(k in f for k in ["ìë£Œ","ìë£Œê´€ë¦¬"]): return "ìë£Œê´€ë¦¬"
        if any(k in f for k in ["ê¸°ë¡","ê¸°ë¡ë¬¼"]): return "ê¸°ë¡ê´€ë¦¬"
        return "í–‰ì •ì •ì±…"
    if domain == "ê²½ì˜ê´€ë¦¬":
        if any(k in f for k in ["ì„±ê³¼","ì„±ê³¼í‰ê°€"]): return "ì„±ê³¼ê´€ë¦¬"
        if any(k in f for k in ["ë‚´ë¶€í†µì œ","í†µì œ"]): return "ë‚´ë¶€í†µì œ"
        if any(k in f for k in ["ì¡°ì§","ì§ì œ"]): return "ì¡°ì§ê´€ë¦¬"
        return "ê²½ì˜ì •ì±…"
    return "ì¼ë°˜"

def infer_doc_level(filename: str) -> str:
    if filename.startswith("1_"): return "ì •ê´€"
    if filename.startswith("2_"): return "ê·œì •"
    if filename.startswith("3_"): return "ê·œì¹™"
    if filename.startswith("4_"): return "ì§€ì¹¨"
    return "ê¸°íƒ€"

def stable_doc_id(file_name: str) -> str:
    return str(uuid5(NAMESPACE_URL, f"doc::{file_name}"))

# -------------------- ì„œì‹ ê´€ë ¨ í•¨ìˆ˜ --------------------

def is_form_page(text: str) -> bool:
    """í˜ì´ì§€ê°€ ì„œì‹ í˜ì´ì§€ì¸ì§€ í™•ì¸"""
    if not text:
        return False
    
    lines = text.split('\n')
    for line in lines[:5]:  # ì²˜ìŒ 5ì¤„ë§Œ í™•ì¸
        line = line.strip()
        for pattern in FORM_START_PATTERNS:
            if pattern.match(line):
                return True
    return False

def extract_form_title(text: str) -> str:
    """ì„œì‹ í˜ì´ì§€ì—ì„œ ì œëª©ì„ ì¶”ì¶œ"""
    if not text:
        return None
    
    lines = text.split('\n')
    
    # ì„œì‹ ì œëª© ì°¾ê¸° (ë³´í†µ 2-5ë²ˆì§¸ ë¼ì¸ì— ìˆìŒ)
    form_title = None
    anchor_raw = None
    
    # ì²« ë²ˆì§¸ ë¼ì¸ì—ì„œ ì•µì»¤ í™•ì¸
    if lines:
        first_line = lines[0].strip()
        for pattern in FORM_START_PATTERNS:
            if pattern.match(first_line):
                anchor_raw = first_line
                break
    
    # ì„œì‹ ì œëª© íŒ¨í„´ì´ í¬í•¨ëœ ë¼ì¸ ì°¾ê¸°
    for i in range(1, min(6, len(lines))):
        line = lines[i].strip()
        if line and len(line) > 2:
            for pattern in FORM_TITLE_PATTERNS:
                if re.search(pattern, line):
                    form_title = line
                    break
            if form_title:
                break
    
    # ì œëª©ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ë¼ì¸ì—ì„œ ì„œì‹ ë²ˆí˜¸ ì œì™¸í•˜ê³  ì¶”ì¶œ
    if not form_title and lines:
        first_line = lines[0].strip()
        # [ë³„ì§€ ì œ1í˜¸ ì„œì‹] íŒ¨í„´ì—ì„œ ì œëª© ë¶€ë¶„ë§Œ ì¶”ì¶œ
        match = re.search(r'\[ë³„ì§€\s*ì œ\d+í˜¸\s*ì„œì‹\]\s*(.+)', first_line)
        if match and match.group(1).strip():
            form_title = match.group(1).strip()
        else:
            # [ë³„í‘œ 1] ì œëª© íŒ¨í„´ì—ì„œ ì œëª© ë¶€ë¶„ë§Œ ì¶”ì¶œ
            match = re.search(r'\[ë³„í‘œ\s*\d*\]\s*(.+)', first_line)
            if match and match.group(1).strip():
                form_title = match.group(1).strip()
            else:
                form_title = first_line
    
    # íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì •ë¦¬
    if form_title:
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°± ì²˜ë¦¬
        form_title = re.sub(r'[^\w\sê°€-í£]', '', form_title)
        form_title = re.sub(r'\s+', '_', form_title.strip())
        # ì—°ì†ëœ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
        form_title = re.sub(r'_+', '_', form_title)
        # ì•ë’¤ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
        form_title = form_title.strip('_')
        form_title = form_title[:50]  # ê¸¸ì´ ì œí•œ
    
    return form_title, anchor_raw

def generate_form_topics_and_synonyms(form_title: str, domain_primary: str) -> Tuple[List[str], List[str]]:
    """ì„œì‹ ì œëª©ê³¼ ë„ë©”ì¸ì„ ê¸°ë°˜ìœ¼ë¡œ í† í”½ê³¼ ë™ì˜ì–´ ìƒì„±"""
    topics = []
    synonyms = []
    
    if not form_title:
        return topics, synonyms
    
    # ì„œì‹ ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    form_title_lower = form_title.lower()
    
    # ë™ì˜ì–´ ì‚¬ì „ì—ì„œ ë§¤ì¹­ë˜ëŠ” í‚¤ì›Œë“œ ì°¾ê¸°
    for keyword, synonym_list in SYNONYM_DICT.items():
        if keyword in form_title_lower:
            topics.append(keyword)
            synonyms.extend(synonym_list)
    
    # ì„œì‹ ì œëª© ìì²´ë¥¼ í† í”½ì— ì¶”ê°€
    topics.append(form_title)
    
    # ë„ë©”ì¸ë³„ ê¸°ë³¸ í† í”½ ì¶”ê°€
    if domain_primary in SYNONYM_DICT:
        topics.append(domain_primary)
        synonyms.extend(SYNONYM_DICT[domain_primary])
    
    # ì¤‘ë³µ ì œê±°
    topics = list(set(topics))
    synonyms = list(set(synonyms))
    
    return topics, synonyms

def find_form_file_uri(file_name: str, form_title: str) -> str:
    """ë¶„ë¦¬ëœ ì„œì‹ PDF íŒŒì¼ì˜ URI ì°¾ê¸°"""
    if not form_title:
        return None
    
    forms_dir = Path(FORMS_DIR)
    if not forms_dir.exists():
        return None
    
    # íŒŒì¼ëª… íŒ¨í„´: ì›ë³¸íŒŒì¼ëª…_ì„œì‹ì œëª©.pdf
    base_name = file_name.replace('.pdf', '')
    search_pattern = f"{base_name}_{form_title}.pdf"
    
    # ì •í™•í•œ ë§¤ì¹­ ì‹œë„
    form_file = forms_dir / search_pattern
    if form_file.exists():
        return f"s3://company_policy/{search_pattern}"
    
    # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
    for form_file in forms_dir.glob(f"{base_name}_*.pdf"):
        if form_title in form_file.name:
            return f"s3://company_policy/{form_file.name}"
    
    return None

# -------------------- Qdrant --------------------

def ensure_collection(client: QdrantClient, force_reset: bool = False):
    """ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ì¬ì„¤ì •"""
    if force_reset:
        try:
            print(f"ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{COLLECTION_NAME}' ì‚­ì œ ì¤‘...")
            client.delete_collection(COLLECTION_NAME)
            print(f"âœ… ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ")
        except Exception:
            pass
    
    existing = [c.name for c in client.get_collections().collections]
    if COLLECTION_NAME not in existing:
        print(f"ğŸ“ ì»¬ë ‰ì…˜ '{COLLECTION_NAME}' ìƒì„± ì¤‘...")
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=EMBED_DIM, distance=Distance.COSINE)
        )
        # payload ì¸ë±ìŠ¤ ìƒì„±(ì—†ëŠ” ê²½ìš°ë§Œ)
        for field, schema in [
            ("document_type","keyword"), ("document_level","keyword"),
            ("domain_primary","keyword"), ("domain_secondary","keyword"),
            ("year","integer"), ("recency_score","integer"),
            ("page","integer"), ("chunk_index","integer"),
            ("register_date_iso","datetime"), ("doc_title","keyword"),
            ("doc_type","keyword"),
            # ì„œì‹ ê´€ë ¨ ì¸ë±ìŠ¤ ì¶”ê°€
            ("form_title","keyword"), ("form_page","integer"),
            ("topics","keyword"), ("synonyms","keyword"),
            ("form_file_uri","keyword"), ("anchor_refs","keyword"),
        ]:
            try:
                client.create_payload_index(COLLECTION_NAME, field_name=field, field_schema=schema)
            except Exception:
                pass
        print(f"âœ… ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")
    else:
        print(f"â„¹ï¸ ì»¬ë ‰ì…˜ '{COLLECTION_NAME}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

# -------------------- ë©”ì¸ íŒŒì´í”„ë¼ì¸ --------------------

def read_pdf_by_page(pdf_path: Path) -> List[Tuple[int, str]]:
    reader = PdfReader(str(pdf_path))
    out = []
    for i, page in enumerate(reader.pages, start=1):
        txt = page.extract_text() or ""
        out.append((i, txt))
    return out

def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[str]:
    # ê°„ë‹¨ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° (langchain splitter ì—†ì´ë„ ì¶©ë¶„)
    if not text:
        return []
    chunks = []
    start = 0
    n = len(text)
    while start < n:
        end = min(n, start + chunk_size)
        chunks.append(text[start:end])
        if end == n:
            break
        start = max(end - chunk_overlap, start + 1)
    return chunks

def main():
    # ëª…ë ¹ì¤„ ì¸ìˆ˜ íŒŒì‹±
    parser = argparse.ArgumentParser(description='PDF ë¬¸ì„œ ì„ë² ë”© ë° Qdrant ì—…ë¡œë“œ')
    parser.add_argument('--reset', '-r', action='store_true', 
                       help='ê¸°ì¡´ ë°ì´í„° ëª¨ë‘ ì‚­ì œí•˜ê³  ìƒˆë¡œ ì‹œì‘')
    args = parser.parse_args()
    
    print("ğŸš€ KoE5 ì„ë² ë”© + Qdrant ì—…ì„œíŠ¸ ì‹œì‘")
    
    if args.reset:
        print("ğŸ”„ ê¸°ì¡´ ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚­ì œí•˜ê³  ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
    else:
        print("âœ… ê¸°ì¡´ ë°ì´í„°ë¥¼ ìœ ì§€í•˜ê³  ìƒˆë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.")
    
    embedder = SentenceTransformer(EMBED_MODEL)
    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT, grpc_port=6334, prefer_grpc=True)
    ensure_collection(client, force_reset=args.reset)

    pdf_dir = Path(PDF_DIR)
    if not pdf_dir.is_dir():
        print(f"âŒ PDF ë””ë ‰í† ë¦¬ ì—†ìŒ: {pdf_dir}")
        return

    pdf_files = sorted([p for p in pdf_dir.glob("*.pdf")])
    if not pdf_files:
        print("âŒ PDF ì—†ìŒ")
        return

    # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì²˜ìŒ 10ê°œ íŒŒì¼ë§Œ ì²˜ë¦¬
    pdf_files = pdf_files[:20]
    print(f"ğŸ“š í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    batch: List[PointStruct] = []

    for pdf_path in tqdm(pdf_files, desc="Index PDFs"):
        file = pdf_path.name
        doc_id = stable_doc_id(file)
        meta_date = parse_register_date_from_filename(file)
        domain_primary = classify_domain_by_filename(file)
        domain_secondary = extract_subdomain_by_filename(file, domain_primary)

        try:
            pages = read_pdf_by_page(pdf_path)
            total_pages = len(pages)

            for page_no, raw_text in pages:
                # ì „ì²˜ë¦¬: í—¤ë” ì œê±° â†’ ë¶ˆí•„ìš”ë¬¸ì ì œê±° â†’ ì²­í¬
                page_text = clean_text(strip_header(raw_text))
                if not page_text:
                    continue
                chunks = [clean_text(c) for c in chunk_text(page_text)]
                if not chunks:
                    continue

                # ë°°ì¹˜ ì„ë² ë”© (ë¬¸ì„œì—” embed_documents)
                vecs = embedder.encode(chunks, batch_size=32, show_progress_bar=False)

                # ê³µí†µ payload
                payload_common: Dict[str, Any] = {
                    "doc_id": doc_id,
                    "doc_title": file.replace(".pdf", ""),
                    "file_path": str(pdf_path),
                    "source": file.replace(".pdf", ""),
                    "document_level": infer_doc_level(file),
                    "document_type": infer_doc_level(file),
                    "domain_primary": domain_primary,
                    "domain_secondary": domain_secondary,
                    "register_date_iso": meta_date["register_date_iso"],
                    "year": meta_date["year"],
                    "month": meta_date["month"],
                    "day": meta_date["day"],
                    "recency_score": calculate_recency_score(meta_date["year"]),
                    "total_pages": total_pages,
                    "file_size": pdf_path.stat().st_size,
                    "embed_backend": EMBED_BACKEND,
                    "embed_model": EMBED_MODEL,
                    "embed_dim": EMBED_DIM,
                }

                # ì„œì‹ í˜ì´ì§€ í™•ì¸
                is_form = is_form_page(raw_text)
                form_title = None
                form_anchor_raw = None
                form_file_uri = None
                topics = []
                synonyms = []
                
                if is_form:
                    form_title, form_anchor_raw = extract_form_title(raw_text)
                    if form_title:
                        form_file_uri = find_form_file_uri(file, form_title)
                        topics, synonyms = generate_form_topics_and_synonyms(form_title, domain_primary)

                total_chunks = len(chunks)
                
                # ì„œì‹ í¬ì¸íŠ¸ ì¶”ê°€ (ì„œì‹ í˜ì´ì§€ì¸ ê²½ìš°, í˜ì´ì§€ë‹¹ í•œ ë²ˆë§Œ)
                if is_form and form_title:
                    # ì„œì‹ ì œëª©ì„ í—¤ë“œë…¸íŠ¸ë¡œ í™œìš©í•˜ì—¬ ì„ë² ë”© ìƒì„±
                    headnote_text = f"[ì„œì‹] {form_title}"
                    if form_anchor_raw:
                        headnote_text = f"{form_anchor_raw} {form_title}"
                    
                    # ì„œì‹ í¬ì¸íŠ¸ìš© ì„ë² ë”© ìƒì„±
                    form_vec = embedder.encode([headnote_text], batch_size=1, show_progress_bar=False)[0]
                    
                    # ì„œì‹ í¬ì¸íŠ¸ payload
                    form_payload = {
                        **payload_common,
                        "text": headnote_text,
                        "page": page_no,
                        "chunk_index": 0,  # ì„œì‹ì€ ë‹¨ì¼ í¬ì¸íŠ¸
                        "total_chunks": 1,
                        "chunk_char_len": len(headnote_text),
                        "doc_type": "form",
                        "form_title": form_title,
                        "form_page": page_no,
                        "form_file_uri": form_file_uri,
                        "form_anchor_raw": form_anchor_raw,
                        "topics": topics,
                        "synonyms": synonyms,
                        "anchor_refs": [f"{doc_id}#ì œ{page_no}ì¡°"] if page_no > 0 else [],
                    }
                    
                    form_point_id = str(uuid5(NAMESPACE_URL, f"{doc_id}:{page_no}:f:0"))
                    batch.append(PointStruct(id=form_point_id, vector=form_vec.tolist() if hasattr(form_vec, 'tolist') else form_vec, payload=form_payload))

                # ê·œì • ì²­í¬ í¬ì¸íŠ¸ë“¤ (ê¸°ì¡´ ë¡œì§)
                for idx, (chunk, vec) in enumerate(zip(chunks, vecs)):
                    payload = {
                        **payload_common,
                        "text": chunk,
                        "page": page_no,
                        "chunk_index": idx,
                        "total_chunks": total_chunks,
                        "chunk_char_len": len(chunk),
                        "doc_type": "text",
                    }
                    point_id = str(uuid5(NAMESPACE_URL, f"{doc_id}:{page_no}:t:{idx}"))
                    batch.append(PointStruct(id=point_id, vector=vec.tolist() if hasattr(vec, 'tolist') else vec, payload=payload))

                    if len(batch) >= BATCH_SIZE:
                        client.upsert(collection_name=COLLECTION_NAME, points=batch)
                        batch.clear()
        except Exception as e:
            print(f"âŒ ì‹¤íŒ¨ {file}: {e}")

    if batch:
        client.upsert(collection_name=COLLECTION_NAME, points=batch)

    info = client.get_collection(COLLECTION_NAME)
    print(f"ğŸ‰ ì™„ë£Œ. points: {getattr(info, 'points_count', 'N/A')}")

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
"""
PDF â†’ Clean Text â†’ Page-aware Chunking â†’ KoE5 Embedding â†’ Qdrant Upsert
- ë¶ˆí•„ìš” ë¬¸ì/ì œì–´ë¬¸ì ì œê±°(clean_text)
- í˜ì´ì§€ ë²ˆí˜¸ ë³´ì¡´ + ì•ˆì •ì  point ID(uuid5)
- í™•ì¥ëœ ë©”íƒ€ë°ì´í„°(payload = metadata)
- ìì£¼ ì“°ëŠ” í•„ë“œ ì¸ë±ìŠ¤ ìƒì„±

ì‚¬ìš©ë²•:
  python embed_documents.py                    # ê¸°ì¡´ ë°ì´í„° ìœ ì§€ (ê¸°ë³¸ê°’)
  python embed_documents.py --reset           # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆë¡œ ì‹œì‘
  python embed_documents.py -r                # --resetì˜ ì¶•ì•½í˜•

ì‚¬ìš© ì „ í™˜ê²½ë³€ìˆ˜(.env í˜¹ì€ ì‹œìŠ¤í…œ í™˜ê²½):
  PDF_DIR=/app/documents/kisa_pdf
  QDRANT_HOST=qdrant
  QDRANT_PORT=6333
  COLLECTION_NAME=regulations_final
  RESET_COLLECTION=false
  BATCH_SIZE=256

í•„ìš” íŒ¨í‚¤ì§€:
  pip install qdrant-client sentence-transformers PyPDF2 tqdm python-dotenv
"""
from __future__ import annotations

import os
import re
import sys
import uuid
import argparse
from uuid import uuid5, NAMESPACE_URL
from pathlib import Path
from typing import Dict, Any, List, Tuple
from datetime import datetime, timezone, timedelta

from dotenv import load_dotenv
from tqdm import tqdm
from PyPDF2 import PdfReader

from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance, PointStruct

# -------------------- í™˜ê²½ --------------------
load_dotenv()
PDF_DIR = os.getenv("PDF_DIR", "/app/documents/kisa_pdf")
QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "regulations_final")
RESET_COLLECTION = os.getenv("RESET_COLLECTION", "false").lower() == "true"
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "256"))

# Embedding: KoE5 (í•œêµ­ì–´ ìµœì í™”, 1024ì°¨ì›)
EMBED_MODEL = os.getenv("HF_MODEL", "nlpai-lab/KoE5")
EMBED_BACKEND = "hf"
EMBED_DIM = 1024

KST = timezone(timedelta(hours=9))

# -------------------- ìœ í‹¸ --------------------
def now_year_kst() -> int:
    return datetime.now(KST).year

def calculate_recency_score(year: int) -> int:
    if not year:
        return 1
    cy = now_year_kst()
    if year >= cy:
        return 3
    elif year >= cy - 2:
        return 2
    else:
        return 1

def parse_register_date_from_filename(filename: str) -> Dict[str, Any]:
    """íŒŒì¼ëª… ë‚´ (YYMMDD) â†’ ISO ë° íŒŒìƒê°’
    ì˜ˆ) 3_16_ì „ìì„œëª…... (210809).pdf â†’ 2021-08-09
    """
    m = re.search(r"\((\d{6})\)", filename)
    if not m:
        return {"register_date_iso": None, "year": 0, "month": 0, "day": 0}
    y, mo, d = m.group(1)[:2], m.group(1)[2:4], m.group(1)[4:6]
    year = 2000 + int(y)
    return {
        "register_date_iso": f"{year:04d}-{int(mo):02d}-{int(d):02d}",
        "year": year,
        "month": int(mo),
        "day": int(d)
    }

# ë¶ˆí•„ìš” ë¬¸ì/ì œì–´ë¬¸ì ì œê±°
_DEF_SYMBOLS = "â–¡â– â—‹â—â—†â—‡â–¶â–·â—€â—â€»â˜†â˜…â€¢ã†âˆ™â—¦â—â–ªï¸â–«ï¸â–âœ“âœ”âœ—âœ˜âŒ"

def clean_text(text: str) -> str:
    if not text:
        return ""
    # Zero-width & ì œì–´ë¬¸ì ì œê±°
    text = re.sub(r"[\u200B-\u200D\uFEFF]", "", text)              # zero-width
    text = re.sub(r"[\x00-\x1f\x7f-\x9f]", " ", text)              # C0/C1 control
    # ë„í˜•/ê¸€ë¨¸ë¦¬ ê¸°í˜¸ ì œê±° (í•„ìš” ì‹œ ì¹˜í™˜ íƒœê¹…ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥)
    text = re.sub(f"[{re.escape(_DEF_SYMBOLS)}]", " ", text)
    # í•˜ì´í”ˆ ì¤„ë°”ê¿ˆ ì—°ê²°(ì›Œë“œë© ì•„í‹°íŒ©íŠ¸)
    text = re.sub(r"-\s*\n\s*", "", text)
    # ì—¬ëŸ¬ ê³µë°±/íƒ­ ì •ê·œí™”
    text = re.sub(r"[ \t]+", " ", text)
    # ì—°ì† ê³µë°± ì¶•ì†Œ
    text = re.sub(r"\s{2,}", " ", text)
    return text.strip()

# ë¨¸ë¦¬ë§ ì œê±° ì •ê·œì‹ ìˆ˜ì •
_DEF_HEADER_RE = re.compile(r"ì œ\s*\d+\s*(ì¥|ì¡°)")

def strip_header(text: str) -> str:
    m = _DEF_HEADER_RE.search(text or "")
    return text[m.start():] if m else (text or "")

# íŒŒì¼ëª… ê¸°ë°˜ ë„ë©”ì¸/ì„œë¸Œë„ë©”ì¸/ë¬¸ì„œíƒ€ì… ì¶”ì •

def classify_domain_by_filename(filename: str) -> str:
    f = filename.casefold()
    if any(k in f for k in ["ì¸ì‚¬","ê¸‰ì—¬","ì±„ìš©","ë³µë¬´","êµìœ¡"]):
        return "ì¸ì‚¬ê´€ë¦¬"
    if any(k in f for k in ["íšŒê³„","ì˜ˆì‚°","ê°ì‚¬","ì¬ì •","ê³„ì•½","ìì‚°"]):
        return "ì¬ë¬´ê´€ë¦¬"
    if any(k in f for k in ["ë³´ì•ˆ","ì •ë³´ë³´í˜¸","ê°œì¸ì •ë³´","ë¯¼ì›"]):
        return "ë³´ì•ˆê´€ë¦¬"
    if any(k in f for k in ["ê¸°ìˆ ","ì •ë³´í™”","ì „ìì„œëª…","ì‹œìŠ¤í…œ","it"]):
        return "ê¸°ìˆ ê´€ë¦¬"
    if any(k in f for k in ["ë¬¸ì„œ","ìë£Œ","ê¸°ë¡","í™ë³´"]):
        return "í–‰ì •ê´€ë¦¬"
    if any(k in f for k in ["ê²½ì˜","ì„±ê³¼","ë‚´ë¶€í†µì œ","ì¡°ì§","ì§ì œ"]):
        return "ê²½ì˜ê´€ë¦¬"
    return "ì¼ë°˜"

def extract_subdomain_by_filename(filename: str, domain: str) -> str:
    f = filename.casefold()
    if domain == "ì¸ì‚¬ê´€ë¦¬":
        if any(k in f for k in ["ê¸‰ì—¬","ë´‰ê¸‰"]): return "ê¸‰ì—¬ê´€ë¦¬"
        if any(k in f for k in ["ì±„ìš©","ì„ìš©"]): return "ì±„ìš©ê´€ë¦¬"
        if any(k in f for k in ["ë³µë¬´","ê·¼ë¬´"]): return "ë³µë¬´ê´€ë¦¬"
        if any(k in f for k in ["êµìœ¡","í›ˆë ¨"]): return "êµìœ¡í›ˆë ¨"
        return "ì¸ì‚¬ì •ì±…"
    if domain == "ì¬ë¬´ê´€ë¦¬":
        if any(k in f for k in ["íšŒê³„","ì¥ë¶€"]): return "íšŒê³„ê´€ë¦¬"
        if any(k in f for k in ["ê°ì‚¬","ê°ì‚¬ì¸"]): return "ê°ì‚¬ê´€ë¦¬"
        if any(k in f for k in ["ê³„ì•½","ê³„ì•½ì‚¬ë¬´"]): return "ê³„ì•½ê´€ë¦¬"
        if any(k in f for k in ["ìì‚°","ë¹„ìœ ë™ìì‚°"]): return "ìì‚°ê´€ë¦¬"
        return "ì¬ë¬´ì •ì±…"
    if domain == "ë³´ì•ˆê´€ë¦¬":
        if any(k in f for k in ["ì •ë³´ë³´í˜¸","ì •ë³´ë³´ì•ˆ"]): return "ì •ë³´ë³´í˜¸"
        if any(k in f for k in ["ê°œì¸ì •ë³´","ê°œì¸ì •ë³´ë³´í˜¸"]): return "ê°œì¸ì •ë³´ë³´í˜¸"
        if any(k in f for k in ["ë¯¼ì›","ì‹ ê³ "]): return "ë¯¼ì›ì‹ ê³ "
        return "ë³´ì•ˆì •ì±…"
    if domain == "ê¸°ìˆ ê´€ë¦¬":
        if any(k in f for k in ["ì •ë³´í™”","ì •ë³´ì‹œìŠ¤í…œ"]): return "ì •ë³´í™”ê´€ë¦¬"
        if any(k in f for k in ["ì „ìì„œëª…","ì¸ì¦"]): return "ì „ìì„œëª…ê´€ë¦¬"
        return "ê¸°ìˆ ì •ì±…"
    if domain == "í–‰ì •ê´€ë¦¬":
        if any(k in f for k in ["ë¬¸ì„œ","ë¬¸ì„œê´€ë¦¬"]): return "ë¬¸ì„œê´€ë¦¬"
        if any(k in f for k in ["ìë£Œ","ìë£Œê´€ë¦¬"]): return "ìë£Œê´€ë¦¬"
        if any(k in f for k in ["ê¸°ë¡","ê¸°ë¡ë¬¼"]): return "ê¸°ë¡ê´€ë¦¬"
        return "í–‰ì •ì •ì±…"
    if domain == "ê²½ì˜ê´€ë¦¬":
        if any(k in f for k in ["ì„±ê³¼","ì„±ê³¼í‰ê°€"]): return "ì„±ê³¼ê´€ë¦¬"
        if any(k in f for k in ["ë‚´ë¶€í†µì œ","í†µì œ"]): return "ë‚´ë¶€í†µì œ"
        if any(k in f for k in ["ì¡°ì§","ì§ì œ"]): return "ì¡°ì§ê´€ë¦¬"
        return "ê²½ì˜ì •ì±…"
    return "ì¼ë°˜"

def infer_doc_level(filename: str) -> str:
    if filename.startswith("1_"): return "ì •ê´€"
    if filename.startswith("2_"): return "ê·œì •"
    if filename.startswith("3_"): return "ê·œì¹™"
    if filename.startswith("4_"): return "ì§€ì¹¨"
    return "ê¸°íƒ€"

def stable_doc_id(file_name: str) -> str:
    return str(uuid5(NAMESPACE_URL, f"doc::{file_name}"))

# -------------------- Qdrant --------------------

def ensure_collection(client: QdrantClient, force_reset: bool = False):
    """ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ì¬ì„¤ì •"""
    if force_reset:
        try:
            print(f"ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{COLLECTION_NAME}' ì‚­ì œ ì¤‘...")
            client.delete_collection(COLLECTION_NAME)
            print(f"âœ… ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ")
        except Exception:
            pass
    
    existing = [c.name for c in client.get_collections().collections]
    if COLLECTION_NAME not in existing:
        print(f"ğŸ“ ì»¬ë ‰ì…˜ '{COLLECTION_NAME}' ìƒì„± ì¤‘...")
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=EMBED_DIM, distance=Distance.COSINE)
        )
        # payload ì¸ë±ìŠ¤ ìƒì„±(ì—†ëŠ” ê²½ìš°ë§Œ)
        for field, schema in [
            ("document_type","keyword"), ("document_level","keyword"),
            ("domain_primary","keyword"), ("domain_secondary","keyword"),
            ("year","integer"), ("recency_score","integer"),
            ("page","integer"), ("chunk_index","integer"),
            ("register_date_iso","datetime"), ("doc_title","keyword"),
            ("doc_type","keyword"),
        ]:
            try:
                client.create_payload_index(COLLECTION_NAME, field_name=field, field_schema=schema)
            except Exception:
                pass
        print(f"âœ… ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")
    else:
        print(f"â„¹ï¸ ì»¬ë ‰ì…˜ '{COLLECTION_NAME}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

# -------------------- ë©”ì¸ íŒŒì´í”„ë¼ì¸ --------------------

def read_pdf_by_page(pdf_path: Path) -> List[Tuple[int, str]]:
    reader = PdfReader(str(pdf_path))
    out = []
    for i, page in enumerate(reader.pages, start=1):
        txt = page.extract_text() or ""
        out.append((i, txt))
    return out

def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[str]:
    # ê°„ë‹¨ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° (langchain splitter ì—†ì´ë„ ì¶©ë¶„)
    if not text:
        return []
    chunks = []
    start = 0
    n = len(text)
    while start < n:
        end = min(n, start + chunk_size)
        chunks.append(text[start:end])
        if end == n:
            break
        start = max(end - chunk_overlap, start + 1)
    return chunks

def main():
    # ëª…ë ¹ì¤„ ì¸ìˆ˜ íŒŒì‹±
    parser = argparse.ArgumentParser(description='PDF ë¬¸ì„œ ì„ë² ë”© ë° Qdrant ì—…ë¡œë“œ')
    parser.add_argument('--reset', '-r', action='store_true', 
                       help='ê¸°ì¡´ ë°ì´í„° ëª¨ë‘ ì‚­ì œí•˜ê³  ìƒˆë¡œ ì‹œì‘')
    args = parser.parse_args()
    
    print("ğŸš€ KoE5 ì„ë² ë”© + Qdrant ì—…ì„œíŠ¸ ì‹œì‘")
    
    if args.reset:
        print("ğŸ”„ ê¸°ì¡´ ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚­ì œí•˜ê³  ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
    else:
        print("âœ… ê¸°ì¡´ ë°ì´í„°ë¥¼ ìœ ì§€í•˜ê³  ìƒˆë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.")
    
    embedder = SentenceTransformer(EMBED_MODEL)
    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT, prefer_grpc=True)
    ensure_collection(client, force_reset=args.reset)

    pdf_dir = Path(PDF_DIR)
    if not pdf_dir.is_dir():
        print(f"âŒ PDF ë””ë ‰í† ë¦¬ ì—†ìŒ: {pdf_dir}")
        return

    pdf_files = sorted([p for p in pdf_dir.glob("*.pdf")])
    if not pdf_files:
        print("âŒ PDF ì—†ìŒ")
        return

    print(f"ğŸ“š ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    batch: List[PointStruct] = []

    for pdf_path in tqdm(pdf_files, desc="Index PDFs"):
        file = pdf_path.name
        doc_id = stable_doc_id(file)
        meta_date = parse_register_date_from_filename(file)
        domain_primary = classify_domain_by_filename(file)
        domain_secondary = extract_subdomain_by_filename(file, domain_primary)

        try:
            pages = read_pdf_by_page(pdf_path)
            total_pages = len(pages)

            for page_no, raw_text in pages:
                # ì „ì²˜ë¦¬: í—¤ë” ì œê±° â†’ ë¶ˆí•„ìš”ë¬¸ì ì œê±° â†’ ì²­í¬
                page_text = clean_text(strip_header(raw_text))
                if not page_text:
                    continue
                chunks = [clean_text(c) for c in chunk_text(page_text)]
                if not chunks:
                    continue

                # ë°°ì¹˜ ì„ë² ë”© (ë¬¸ì„œì—” embed_documents)
                vecs = embedder.encode(chunks, batch_size=32, show_progress_bar=False)

                # ê³µí†µ payload
                payload_common: Dict[str, Any] = {
                    "doc_id": doc_id,
                    "doc_title": file.replace(".pdf", ""),
                    "file_path": str(pdf_path),
                    "source": file.replace(".pdf", ""),
                    "document_level": infer_doc_level(file),
                    "document_type": infer_doc_level(file),
                    "domain_primary": domain_primary,
                    "domain_secondary": domain_secondary,
                    "register_date_iso": meta_date["register_date_iso"],
                    "year": meta_date["year"],
                    "month": meta_date["month"],
                    "day": meta_date["day"],
                    "recency_score": calculate_recency_score(meta_date["year"]),
                    "total_pages": total_pages,
                    "file_size": pdf_path.stat().st_size,
                    "embed_backend": EMBED_BACKEND,
                    "embed_model": EMBED_MODEL,
                    "embed_dim": EMBED_DIM,
                }

                total_chunks = len(chunks)
                for idx, (chunk, vec) in enumerate(zip(chunks, vecs)):
                    payload = {
                        **payload_common,
                        "text": chunk,
                        "page": page_no,
                        "chunk_index": idx,
                        "total_chunks": total_chunks,
                        "chunk_char_len": len(chunk),
                        "doc_type": "text",
                    }
                    point_id = str(uuid5(NAMESPACE_URL, f"{doc_id}:{page_no}:t:{idx}"))
                    batch.append(PointStruct(id=point_id, vector=vec.tolist() if hasattr(vec, 'tolist') else vec, payload=payload))

                    if len(batch) >= BATCH_SIZE:
                        client.upsert(collection_name=COLLECTION_NAME, points=batch)
                        batch.clear()
        except Exception as e:
            print(f"âŒ ì‹¤íŒ¨ {file}: {e}")

    if batch:
        client.upsert(collection_name=COLLECTION_NAME, points=batch)

    info = client.get_collection(COLLECTION_NAME)
    print(f"ğŸ‰ ì™„ë£Œ. points: {getattr(info, 'points_count', 'N/A')}")

if __name__ == "__main__":
    main() 